{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KDrama_Scraping.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Theory\n",
        "\n",
        "1. Install packages and libraries for `webdriver`.\n",
        "2. Review the webpage's `HTML` structure and determine the path to obtain the data we want to scrape.\n",
        "3. Start scraping the data using `Selenium` and `pandas`.\n",
        "4. Output the processed data into a `CSV` file.\n",
        "\n",
        "The `ipynb` below is run on [Google Colab](https://colab.research.google.com/?utm_source=scs-index) as the IDE contains multiple built-in libraries such as `pandas` and many more. It also runs on LinuxOS, which allows more functions that a WinOS user cannot work with.\n",
        "\n",
        "[Link to the Colab Notebook](https://colab.research.google.com/drive/1bsTGqjkcSL4J3TJJnDVxGwPUmsQwHFFQ?usp=sharing)"
      ],
      "metadata": {
        "id": "2Xam82twqORq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Packages"
      ],
      "metadata": {
        "id": "xlOdVgFrqo_N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s53ay_WyqGvS",
        "outputId": "1cac9a44-df80-483f-bbac-4c68518b1a8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting selenium\n",
            "  Downloading selenium-4.3.0-py3-none-any.whl (981 kB)\n",
            "\u001b[K     |████████████████████████████████| 981 kB 5.2 MB/s \n",
            "\u001b[?25hCollecting trio~=0.17\n",
            "  Downloading trio-0.21.0-py3-none-any.whl (358 kB)\n",
            "\u001b[K     |████████████████████████████████| 358 kB 34.3 MB/s \n",
            "\u001b[?25hCollecting trio-websocket~=0.9\n",
            "  Downloading trio_websocket-0.9.2-py3-none-any.whl (16 kB)\n",
            "Collecting urllib3[secure,socks]~=1.26\n",
            "  Downloading urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 60.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (21.4.0)\n",
            "Collecting async-generator>=1.9\n",
            "  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Collecting sniffio\n",
            "  Downloading sniffio-1.2.0-py3-none-any.whl (10 kB)\n",
            "Collecting outcome\n",
            "  Downloading outcome-1.2.0-py2.py3-none-any.whl (9.7 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.10)\n",
            "Collecting wsproto>=0.14\n",
            "  Downloading wsproto-1.1.0-py3-none-any.whl (24 kB)\n",
            "Collecting pyOpenSSL>=0.14\n",
            "  Downloading pyOpenSSL-22.0.0-py2.py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 3.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from urllib3[secure,socks]~=1.26->selenium) (2022.6.15)\n",
            "Collecting cryptography>=1.3.4\n",
            "  Downloading cryptography-37.0.2-cp36-abi3-manylinux_2_24_x86_64.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 32.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from urllib3[secure,socks]~=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=1.3.4->urllib3[secure,socks]~=1.26->selenium) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=1.3.4->urllib3[secure,socks]~=1.26->selenium) (2.21)\n",
            "Collecting h11<1,>=0.9.0\n",
            "  Downloading h11-0.13.0-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 4.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from h11<1,>=0.9.0->wsproto>=0.14->trio-websocket~=0.9->selenium) (4.1.1)\n",
            "Installing collected packages: sniffio, outcome, h11, cryptography, async-generator, wsproto, urllib3, trio, pyOpenSSL, trio-websocket, selenium\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "requests 2.23.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you have urllib3 1.26.9 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed async-generator-1.10 cryptography-37.0.2 h11-0.13.0 outcome-1.2.0 pyOpenSSL-22.0.0 selenium-4.3.0 sniffio-1.2.0 trio-0.21.0 trio-websocket-0.9.2 urllib3-1.26.9 wsproto-1.1.0\n",
            "Get:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:3 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Ign:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:7 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:9 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Hit:11 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Get:12 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [1,013 kB]\n",
            "Get:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [21.3 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [22.8 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,521 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,866 kB]\n",
            "Get:18 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [2,026 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [1,047 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [29.8 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,298 kB]\n",
            "Get:22 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [1,037 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,297 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic-backports/main amd64 Packages [12.2 kB]\n",
            "Get:25 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [47.8 kB]\n",
            "Fetched 15.5 MB in 4s (4,024 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-codecs-ffmpeg-extra\n",
            "Suggested packages:\n",
            "  webaccounts-chromium-extension unity-chromium-extension\n",
            "The following NEW packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-chromedriver\n",
            "  chromium-codecs-ffmpeg-extra\n",
            "0 upgraded, 4 newly installed, 0 to remove and 57 not upgraded.\n",
            "Need to get 89.8 MB of archives.\n",
            "After this operation, 302 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-codecs-ffmpeg-extra amd64 101.0.4951.64-0ubuntu0.18.04.1 [1,142 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser amd64 101.0.4951.64-0ubuntu0.18.04.1 [78.5 MB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser-l10n all 101.0.4951.64-0ubuntu0.18.04.1 [4,980 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-chromedriver amd64 101.0.4951.64-0ubuntu0.18.04.1 [5,153 kB]\n",
            "Fetched 89.8 MB in 4s (23.7 MB/s)\n",
            "Selecting previously unselected package chromium-codecs-ffmpeg-extra.\n",
            "(Reading database ... 155639 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-codecs-ffmpeg-extra_101.0.4951.64-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-codecs-ffmpeg-extra (101.0.4951.64-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser.\n",
            "Preparing to unpack .../chromium-browser_101.0.4951.64-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-browser (101.0.4951.64-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser-l10n.\n",
            "Preparing to unpack .../chromium-browser-l10n_101.0.4951.64-0ubuntu0.18.04.1_all.deb ...\n",
            "Unpacking chromium-browser-l10n (101.0.4951.64-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_101.0.4951.64-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (101.0.4951.64-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-codecs-ffmpeg-extra (101.0.4951.64-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser (101.0.4951.64-0ubuntu0.18.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (101.0.4951.64-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser-l10n (101.0.4951.64-0ubuntu0.18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: DeprecationWarning: use options instead of chrome_options\n"
          ]
        }
      ],
      "source": [
        "# Install the package.\n",
        "!pip install selenium\n",
        "\n",
        "# Import the required libraries.\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.webdriver.common.by import By\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Install the chrome web driver from selenium. \n",
        "!apt-get update \n",
        "!apt install chromium-chromedriver\n",
        "\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "driver = webdriver.Chrome('chromedriver', chrome_options = chrome_options)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The web driver is a key component of selenium. The web driver is a browser automation framework that works with open source APIs. The framework operates by accepting commands, sending those commands to a browser, and interacting with applications.\n",
        "\n",
        "Selenium supports multiple web browsers and offers web drivers for each browser. I have imported the chrome web driver from selenium. Alternatively, you can download the web driver for your specific browser and store it in a location where it can be easily accessed (C:\\users\\webdriver\\chromedriver.exe). You can download a web driver for your browser at [this site](https://selenium-python.readthedocs.io/installation.html#:~:text=Selenium%20requires%20a-,driver,-to%20interface%20with)."
      ],
      "metadata": {
        "id": "W_fswuFLqwQO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Review Web Page's `HTML` Structure\n",
        "\n",
        "Things to scrape:\n",
        "\n",
        "| Item | Common Structure |\n",
        "| :--- | :--- |\n",
        "| Title | div (ptam-block-post-grid-text) > h6 (ptam-block-post-grid-title) > a (local-link) |\n",
        "| Date | div (ptam-block-post-grid-text) > div(ptam-block-post-grid-byline) > time (ptam-block-post-grid-date) |\n",
        "| Site URL | div (ptam-block-post-grid-image) > a (local-link) > \"href\" |\n",
        "| Image URL (Optional) | div (ptam-block-post-grid-image) > a (local-link) > img (attachment-medium size-medium jetpack-lazy-image jetpack-lazy-image--handled lazyloaded) > \"src\" |"
      ],
      "metadata": {
        "id": "m-soKMKiq2n4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Try Page 1\n",
        "\n",
        "1. Set up scraper at main page."
      ],
      "metadata": {
        "id": "-n845BnEQjvB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Navigate to main page\n",
        "driver.get('https://butchixanh.com/')\n",
        "page = 1"
      ],
      "metadata": {
        "id": "5BtGRe1rtRDN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Scrape data in page 1."
      ],
      "metadata": {
        "id": "0VA3VXndS724"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get elements from HTML\n",
        "raw_titles = driver.find_elements(By.XPATH, \"(//div[@class='ptam-block-post-grid-text']/h6[@class='ptam-block-post-grid-title']/a[@class='local-link'])\")\n",
        "\n",
        "# Copy elements into list\n",
        "movie_titles = []\n",
        "for title in raw_titles: movie_titles.append(title.text)\n",
        "\n",
        "print(movie_titles)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvEBqFrxtXn8",
        "outputId": "14565bf7-a3e0-403a-dbfe-2fec55ea7169"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Money Heist: Korea – Joint Economic Area', 'Cafe Minamdang', 'Item', 'Brilliant Legacy', 'Alchemy of Souls', 'The Witch Is Alive', 'It’s Beautiful Now', 'Miracle', 'Ultimate Weapon Alice', 'Anna', 'Why Her?', 'Doctor Lawyer', 'Yumi’s Cells 2', 'There is No Goo Pil-Soo', 'Jejungwon', 'Scandal in Old Seoul', 'Father, I’ll Take Care of You', 'Eve', 'Insider', 'Jinxed at First']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get date elements from HTML\n",
        "raw_dates = driver.find_elements(By.CLASS_NAME, \"ptam-block-post-grid-date\")\n",
        "\n",
        "# Copy elements into list\n",
        "dates = []\n",
        "for date in raw_dates: dates.append(date.text)\n",
        "\n",
        "print(dates)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUCQ3qJRuZr5",
        "outputId": "fc8363bd-b0ff-4c4a-d3ef-09527179b0d8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['JUNE 24, 2022', 'JUNE 27, 2022', 'JUNE 27, 2022', 'JUNE 27, 2022', 'JUNE 25, 2022', 'JUNE 25, 2022', 'JUNE 25, 2022', 'JUNE 25, 2022', 'JUNE 25, 2022', 'JUNE 25, 2022', 'JUNE 24, 2022', 'JUNE 24, 2022', 'JUNE 24, 2022', 'JUNE 24, 2022', 'JUNE 23, 2022', 'JUNE 23, 2022', 'JUNE 23, 2022', 'JUNE 23, 2022', 'JUNE 22, 2022', 'JUNE 22, 2022']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get site url attributes from HTML\n",
        "raw_site_urls = driver.find_elements(By.XPATH, \"//div[@class='ptam-block-post-grid-image']/a\")\n",
        "\n",
        "# Copy elements into list\n",
        "site_urls = []\n",
        "for site in raw_site_urls: site_urls.append(site.get_attribute('href'))\n",
        "\n",
        "print(site_urls)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olMt0MOHNXXQ",
        "outputId": "b61a2128-bd43-4273-b3ec-9f694e4d16ce"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['https://butchixanh.com/money-heist-korea-joint-economic-area/', 'https://butchixanh.com/cafe-minamdang/', 'https://butchixanh.com/item/', 'https://butchixanh.com/brilliant-legacy/', 'https://butchixanh.com/alchemy-of-souls/', 'https://butchixanh.com/the-witch-is-alive/', 'https://butchixanh.com/the-present-is-beautiful/', 'https://butchixanh.com/miracle-2/', 'https://butchixanh.com/ultimate-weapon-alice/', 'https://butchixanh.com/anna/', 'https://butchixanh.com/why-her/', 'https://butchixanh.com/doctor-lawyer/', 'https://butchixanh.com/yumis-cells-2/', 'https://butchixanh.com/there-is-no-goo-pil-soo/', 'https://butchixanh.com/jejungwon/', 'https://butchixanh.com/scandal-in-old-seoul/', 'https://butchixanh.com/father-ill-take-care-of-you/', 'https://butchixanh.com/eve/', 'https://butchixanh.com/insider/', 'https://butchixanh.com/jinxs-lover/']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Missing Value Check"
      ],
      "metadata": {
        "id": "O3PdfJZ_S_xA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if data counts are same\n",
        "print(len(movie_titles), len(dates), len(site_urls))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MH5PO681wbMi",
        "outputId": "76561084-fade-4788-add3-e69298158b3b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20 20 20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Continue From Page 2 to Page 67"
      ],
      "metadata": {
        "id": "9Q4lBpMRQh6v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Repeatedly loop the following:\n",
        "1. Relocate scraper\n",
        "2. Append scraped data\n",
        "3. Change URL (in this case is change page number at the end of URL)"
      ],
      "metadata": {
        "id": "XRmVIPikTIIv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "while page < 67:\n",
        "  \n",
        "  # Flip Page\n",
        "  page += 1\n",
        "  new_url = 'https://butchixanh.com/page/' + str(page)\n",
        "  driver.get(new_url)\n",
        "\n",
        "  # Clear Medium-List\n",
        "  raw_titles = []\n",
        "  raw_dates = []\n",
        "  raw_site_urls = []\n",
        "  \n",
        "  # Get Titles\n",
        "  raw_titles = driver.find_elements(By.XPATH, \"(//div[@class='ptam-block-post-grid-text']/h6[@class='ptam-block-post-grid-title']/a[@class='local-link'])\")\n",
        "  for title in raw_titles: movie_titles.append(title.text)\n",
        "\n",
        "  # Get Dates\n",
        "  raw_dates = driver.find_elements(By.CLASS_NAME, \"ptam-block-post-grid-date\")\n",
        "  for date in raw_dates: dates.append(date.text)\n",
        "\n",
        "  # Get Site URL\n",
        "  raw_site_urls = driver.find_elements(By.XPATH, \"//div[@class='ptam-block-post-grid-image']/a\")\n",
        "  for site in raw_site_urls: site_urls.append(site.get_attribute('href'))\n",
        "\n",
        "print(len(movie_titles), len(dates), len(site_urls))"
      ],
      "metadata": {
        "id": "cDeqBgX790Gl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92b006a4-1778-4402-aa62-af8ba12d133c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1339 1339 1339\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is great here that all `movie_titles`, `dates` and `site_urls` have complete amount of non-`Null` values, which is 1339 in this case. One down side of this scraping method is that if there are `Null` values, no value will be return (it will skip it completely) instead of `None`. Therefore, this will cause the whole list to be offset, which is not ideal when we want to convert it into tabular data."
      ],
      "metadata": {
        "id": "S2KV4n3IF8Sv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert Scraped Data"
      ],
      "metadata": {
        "id": "XdCE4CabScJT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will combine the lists created above using `zip()`, and then `list()` to convert it into a single 2D list. Then, we will create a `pandas` Data Frame to show us our tabular data."
      ],
      "metadata": {
        "id": "oe-aMbU8FRi5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine lists\n",
        "data = list(zip(movie_titles, dates, site_urls))\n",
        "\n",
        "# Create Dataframe\n",
        "df = pd.DataFrame(data, columns=['Movie Titles', 'Post Date', 'Site URL'])\n",
        "print(df)"
      ],
      "metadata": {
        "id": "M05NyyGMKbl8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "763e2123-5aa1-4fd1-cd35-fac692dc2525"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                  Movie Titles        Post Date  \\\n",
            "0     Money Heist: Korea – Joint Economic Area    JUNE 24, 2022   \n",
            "1                               Cafe Minamdang    JUNE 27, 2022   \n",
            "2                                         Item    JUNE 27, 2022   \n",
            "3                             Brilliant Legacy    JUNE 27, 2022   \n",
            "4                             Alchemy of Souls    JUNE 25, 2022   \n",
            "...                                        ...              ...   \n",
            "1334                                   Watcher  AUGUST 26, 2019   \n",
            "1335             Love Affairs in the Afternoon  AUGUST 25, 2019   \n",
            "1336                        The King’s Letters  AUGUST 21, 2019   \n",
            "1337              Designated Survivor: 60 Days  AUGUST 20, 2019   \n",
            "1338                                  Parasite  AUGUST 20, 2019   \n",
            "\n",
            "                                               Site URL  \n",
            "0     https://butchixanh.com/money-heist-korea-joint...  \n",
            "1                https://butchixanh.com/cafe-minamdang/  \n",
            "2                          https://butchixanh.com/item/  \n",
            "3              https://butchixanh.com/brilliant-legacy/  \n",
            "4              https://butchixanh.com/alchemy-of-souls/  \n",
            "...                                                 ...  \n",
            "1334                    https://butchixanh.com/watcher/  \n",
            "1335  https://butchixanh.com/love-affairs-in-the-aft...  \n",
            "1336          https://butchixanh.com/the-kings-letters/  \n",
            "1337  https://butchixanh.com/designated-survivor-60-...  \n",
            "1338                   https://butchixanh.com/parasite/  \n",
            "\n",
            "[1339 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we have the option to convert the Data Frame into a `CSV` file for analytical purposes."
      ],
      "metadata": {
        "id": "khqkjQPzGmj-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('Korean_Drama.csv', index = False)"
      ],
      "metadata": {
        "id": "e_pNENg4Snbv"
      },
      "execution_count": 12,
      "outputs": []
    }
  ]
}